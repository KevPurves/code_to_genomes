# SLURM scheduling and download SRA Files Script
This is a guide on how to set up a SLURM script to fetch SRA files and convert them to fastq format using the SRA Toolkit. We will go through the configuration of a SLURM script and how to use the SRA toolkit. 

## What is SLURM
A Slurm system, also known as a Slurm cluster, is a cluster computing system that utilizes the Slurm workload manager (Simple Linux Utility for Resource Management) for job scheduling, resource management, and job accounting on a Linux-based high-performance computing (HPC) cluster. Slurm is one of the most widely used workload managers in the HPC community due to its scalability, flexibility, and robustness.

## Slurm Queue:
A Slurm queue, also known as a job queue, is a logical container within the Slurm workload manager where jobs are submitted and managed. It acts as a waiting area for pending jobs before they are scheduled and executed on compute nodes.

## SLURM Configuration
A typical SLRUM configuration for this script is as follows:
- **Job Name:** download_sra
- **Email Notification:** END,FAIL
- **Email Address:** amy.fitzpatrick@ucd.ie
- **Time Limit:** 0-4:00
- **CPUs per Task:** 10
- **Error Log:** /home/people/fitzpatria/scratch/Filoviridae/logs/fetch_sra_err_%j.log
- **Output Log:** /home/people/fitzpatria/scratch/Filoviridae/logs/fetch_sra_%j.log

If you wish to find the SBATCH options for a specific SLURM configuration, you can use the following command:

```bash
scontrol show config
```
There are many advanced options for setting up SLURM scripts. We will cover them later in the course. Typically bash scripts start with `#!/bin/bash` and are saved with the extension `.sh`. 

To run a bash script and send it to the SLURM queue we use a command `sbatch`. `sbatch` is a command-line tool used in Slurm and is used to submit batch jobs to the Slurm scheduler for execution on the cluster. To check the status of our (user specific) job that we send to the Sonic HPC, we can use the command `squeue -u <username?>`. This will show us something like the following:

```bash
 squeue -u fitzpatria
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
   2370776_[5-7%1]    shared zip_sra_ fitzpatr PD       0:00      1 (JobArrayTaskLimit)
         2370776_4    shared zip_sra_ fitzpatr  R    1:47:50      1 sonic45
```

This information tells us that the user fitzpatria has two jobs in the queue. One is pending (PD) and the other is running (R). The running job is using the node sonic45. The name of the current job is shared zip_sra_ and the job ID is 2370776. The underscore in the JOBID indicates that this is an array job as indiciated by [5-7%1]. Array jobs are a way to submit a collection of similar jobs to the Slurm scheduler. The %1 indicates that the array job is using sending one component of the job at a time to HPC. 

# SRA Toolkit
SRA Toolkit is a collection of tools and libraries for using data in the INSDC Sequence Read Archives. The toolkit includes three main components:
- **SRA Run Selector:** A web-based tool for finding and selecting relevant SRA runs
- **SRA Toolkit:** A set of utilities for accessing data in the SRA format
- **SRA Toolkit Development:** A set of libraries for building tools using the SRA format

## SRA Run Selector
The SRA run selector can be used to download SRA files from the NCBI SRA database. The SRA run selector can be found at the following link: https://www.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA739543&o=acc_s%3Aa. 

We can also install the SRA Toolkit on our local machine. To do this we can use the following command:

```bash
conda create --prefix /home/people/<username>/scratch/tools/sra_tools
conda activate /home/people/<username>/tools/scratch/sra_tools
conda install -c bioconda sra-tools
```

## Task 1- Write a SLURM script to donwload SRA files. 
I have provided the overall structure of the script below. You will need to fill in the missing parts. Remember this should be a SLURM script that you can send to the queue using sbatch. 

In particular, try to take to figure out how the for loop works. For loops are commonly used in bash scripts. They are used to iterate over a sequence of values. In this case, we are iterating over a list of SRA accessions. The list of SRA accessions is defined at the top of the script.

### Hints for the script. 
Define the list of SRA accessions that you wished to download. I and Kevin (thank you!) have provided a list of SRA accessions that you can use. They are ONT runs that have provided their fastq files, and have associated publications. The file is stored in the data folder: training/wk2/data/sra_metadata.csv

If you wish to pick inside the script on the command line you can commands such as `head` and `tail` to view the first and last few lines of the file. If you want to read the entire file you can use the command `cat`. 
 
```bash
SRAs=("X" "Y" "Z")
```
Specify the output directory for fastq-dump. Fastq-dump is a component of the SRA Toolkit that can be used to convert SRA files to fastq format. 

```bash
OUTPUT_DIR="?"
```
Loop through the SRA accessions, download, and convert to fastq using prefetch and fastq-dump.

```bash
for SRA in "${SRAs[@]}"
do
    # Create a directory for each SRA accession
    mkdir -p "$OUTPUT_DIR/$SRA"
    
    # Change to the directory for the specific SRA accession
    cd "$OUTPUT_DIR/$SRA"
    
    # Download the SRA file using prefetch (part of SRA Toolkit)
    prefetch "$SRA"
    
    # Convert the SRA file to fastq format using fastq-dump (part of SRA Toolkit)
    fasterq-dump --split-files "$SRA" -O "$OUTPUT_DIR/$SRA"
    
    echo "Downloaded $SRA successfully"
done
```